# 实现迷你版大模型：mini-mind
1.入门：参考Llama3模型结构，手动搭建一个小型大语言模型mini-mind（MHA多头注意力机制）：分词器训练，模型预训练，SFT微调，DPO后训练的完整流程。<br>
2.(未完成)进阶：参考DeepSeek模型结构，构建我们的mini-deepseek（MoE混合专家结构，MLA多头潜在注意力机制，MTP多token预测），使用DPO进行强化学习微调。<br>

参考：<br>
https://github.com/jingyaogong/minimind<br>
https://github.com/wyf3/llm_related<br>
